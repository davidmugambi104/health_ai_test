# """
# Example Request:
# POST /predict
# {
#   "symptoms": ["fever", "cough", "headache"],
#   "location": "New York, NY",
#   "enable_enrichment": true,
#   "enable_ai_reranking": true
# }

# Example Response (truncated):
# {
#   "success": true,
#   "data": {
#     "input_symptoms": ["Fever", "Cough", "Headache"],
#     "health_score": 75.5,
#     "predictions": [
#       {
#         "disease": "Common Cold",
#         "confidence": 85.2,
#         "ai_confidence": 92.1,
#         "matched_symptoms": ["fever", "cough"],
#         "external_data": { ... },
#         "ai_insights": {
#           "feature_importance": { "fever": 0.35, "cough": 0.28, ... },
#           "case_similarity": 0.76
#         }
#       }
#     ]
#   }
# }
# """

# from flask import Flask, request, jsonify, send_from_directory, render_template
# from flask_cors import CORS
# import pandas as pd
# import re
# from rapidfuzz import process, fuzz
# import numpy as np
# from typing import List, Dict, Set, Tuple, Optional, Any
# import logging
# import os
# from pathlib import Path
# import requests
# import json
# from datetime import datetime, timedelta
# import time
# from dataclasses import dataclass
# from concurrent.futures import ThreadPoolExecutor, as_completed
# import hashlib
# from collections import defaultdict
# import pickle
# from sklearn.preprocessing import LabelEncoder
# import warnings
# warnings.filterwarnings('ignore')

# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)

# app = Flask(__name__)
# CORS(app)

# # Constants
# MIN_SYMPTOMS_REQUIRED = 1
# MAX_SYMPTOMS_ALLOWED = 10
# MIN_MATCHED_SYMPTOMS = 1
# MIN_CONFIDENCE_THRESHOLD = 10.0
# TOP_N_DISEASES = 5
# FUZZY_MATCH_THRESHOLD = 60

# @dataclass
# class EnrichmentData:
#     """Container for enriched disease data from multiple sources"""
#     icd11_data: Optional[Dict[str, Any]] = None
#     clinical_trials: List[Dict[str, Any]] = None
#     drug_interactions: List[Dict[str, Any]] = None
#     genetic_markers: List[Dict[str, Any]] = None
#     environmental_factors: Optional[Dict[str, Any]] = None
#     patient_insights: List[Dict[str, Any]] = None
    
#     def __post_init__(self):
#         if self.clinical_trials is None:
#             self.clinical_trials = []
#         if self.drug_interactions is None:
#             self.drug_interactions = []
#         if self.genetic_markers is None:
#             self.genetic_markers = []
#         if self.patient_insights is None:
#             self.patient_insights = []

# class AIDiagnosticEngine:
#     """
#     AI-powered diagnostic engine for symptom analysis and disease prediction
#     Uses placeholder models that can be replaced with real ML models
#     """
    
#     def __init__(self):
#         self.symptom_encoder = None
#         self.disease_encoder = None
#         self.case_base = {}  # Simple case-based reasoning storage
#         self.model_weights = None
#         self._initialize_placeholders()
#         logger.info("AI Diagnostic Engine initialized")
    
#     def _initialize_placeholders(self):
#         """Initialize placeholder models and encoders"""
#         # Placeholder for BERT-like symptom encoder
#         self.symptom_embedding_dim = 384  # Similar to MiniLM
#         self.disease_embedding_dim = 256
        
#         # Placeholder model weights (simulating a trained model)
#         self.model_weights = {
#             'symptom_importance': np.random.randn(100, 1),  # 100 top symptoms
#             'disease_weights': np.random.randn(50, 1),      # 50 top diseases
#             'interaction_terms': np.random.randn(20, 1)     # Symptom-disease interactions
#         }
        
#         # Initialize case base with some example cases
#         self._initialize_case_base()
    
#     def _initialize_case_base(self):
#         """Initialize case-based reasoning database with sample cases"""
#         self.case_base = {
#             'case_001': {
#                 'symptoms': ['fever', 'cough', 'fatigue'],
#                 'disease': 'influenza',
#                 'confidence': 0.89,
#                 'demographics': {'age': 35, 'gender': 'M'},
#                 'outcome': 'recovered'
#             },
#             'case_002': {
#                 'symptoms': ['headache', 'nausea', 'dizziness'],
#                 'disease': 'migraine',
#                 'confidence': 0.92,
#                 'demographics': {'age': 28, 'gender': 'F'},
#                 'outcome': 'managed'
#             },
#             'case_003': {
#                 'symptoms': ['chest_pain', 'shortness_of_breath'],
#                 'disease': 'angina',
#                 'confidence': 0.95,
#                 'demographics': {'age': 65, 'gender': 'M'},
#                 'outcome': 'treated'
#             }
#         }
    
#     def encode_symptoms(self, symptoms: List[str]) -> np.ndarray:
#         """
#         Encode symptoms into fixed-dimensional vectors (placeholder BERT encoder)
#         In production, this would use a real transformer model
#         """
#         # Create deterministic embeddings based on symptom hashes
#         embeddings = []
#         for symptom in symptoms:
#             # Create deterministic "embedding" based on symptom name
#             symptom_hash = hashlib.md5(symptom.encode()).hexdigest()
#             hash_int = int(symptom_hash[:8], 16)  # Use first 8 chars for determinism
            
#             # Generate pseudo-random but deterministic vector
#             np.random.seed(hash_int % 10000)
#             embedding = np.random.randn(self.symptom_embedding_dim)
#             embedding = embedding / np.linalg.norm(embedding)  # Normalize
#             embeddings.append(embedding)
        
#         if embeddings:
#             # Average pooling of symptom embeddings
#             return np.mean(embeddings, axis=0)
#         else:
#             return np.zeros(self.symptom_embedding_dim)
    
#     def calculate_feature_importance(self, symptoms: List[str], disease: str) -> Dict[str, float]:
#         """
#         Calculate importance of each symptom for the given disease
#         Placeholder implementation - would use SHAP values in production
#         """
#         importance_scores = {}
#         base_importance = 1.0 / len(symptoms) if symptoms else 0
        
#         for symptom in symptoms:
#             # Simulate symptom-disease specific importance
#             symptom_hash = sum(ord(c) for c in symptom) % 100
#             disease_hash = sum(ord(c) for c in disease) % 100
#             combined_hash = (symptom_hash + disease_hash) % 100
            
#             # Generate importance score based on combined hash
#             importance = base_importance * (1 + combined_hash / 100)
#             importance_scores[symptom] = round(importance, 3)
        
#         # Normalize to sum to 1.0
#         total = sum(importance_scores.values())
#         if total > 0:
#             importance_scores = {k: v/total for k, v in importance_scores.items()}
        
#         return importance_scores
    
#     def find_similar_cases(self, symptoms: List[str], demographics: Dict = None) -> List[Dict]:
#         """
#         Find similar historical cases using case-based reasoning
#         """
#         similarities = []
        
#         for case_id, case in self.case_base.items():
#             # Calculate Jaccard similarity between symptom sets
#             case_symptoms = set(case['symptoms'])
#             input_symptoms = set(symptoms)
            
#             intersection = len(case_symptoms.intersection(input_symptoms))
#             union = len(case_symptoms.union(input_symptoms))
            
#             similarity = intersection / union if union > 0 else 0
            
#             # Adjust for demographic similarity if available
#             if demographics and 'demographics' in case:
#                 demo_similarity = self._calculate_demographic_similarity(
#                     demographics, case['demographics']
#                 )
#                 similarity = 0.7 * similarity + 0.3 * demo_similarity
            
#             if similarity > 0.3:  # Only return reasonably similar cases
#                 similarities.append({
#                     'case_id': case_id,
#                     'similarity': round(similarity, 3),
#                     'disease': case['disease'],
#                     'matched_symptoms': list(case_symptoms.intersection(input_symptoms)),
#                     'outcome': case.get('outcome', 'unknown')
#                 })
        
#         # Sort by similarity descending
#         similarities.sort(key=lambda x: x['similarity'], reverse=True)
#         return similarities[:5]  # Return top 5 similar cases
    
#     def _calculate_demographic_similarity(self, demo1: Dict, demo2: Dict) -> float:
#         """Calculate demographic similarity between two cases"""
#         similarity = 0.0
#         factors = 0
        
#         if 'age' in demo1 and 'age' in demo2:
#             age_diff = abs(demo1['age'] - demo2['age'])
#             age_similarity = max(0, 1 - age_diff / 50)  # 50 years max difference
#             similarity += age_similarity
#             factors += 1
        
#         if 'gender' in demo1 and 'gender' in demo2:
#             gender_similarity = 1.0 if demo1['gender'] == demo2['gender'] else 0.3
#             similarity += gender_similarity
#             factors += 1
        
#         return similarity / factors if factors > 0 else 0.5
    
#     def predict_ai_confidence(self, symptoms: List[str], disease: str, 
#                             base_confidence: float, matched_symptoms: List[str]) -> Dict[str, Any]:
#         """
#         Predict AI-enhanced confidence score using placeholder model
#         """
#         # Encode symptoms
#         symptom_embedding = self.encode_symptoms(symptoms)
        
#         # Calculate feature importance
#         feature_importance = self.calculate_feature_importance(matched_symptoms, disease)
        
#         # Find similar cases
#         similar_cases = self.find_similar_cases(symptoms)
        
#         # Simulate model prediction (placeholder logic)
#         symptom_count_factor = len(matched_symptoms) / max(len(symptoms), 1)
#         feature_importance_factor = sum(feature_importance.values()) / len(feature_importance) if feature_importance else 0
        
#         # Case-based adjustment
#         case_similarity = max([case['similarity'] for case in similar_cases]) if similar_cases else 0.5
        
#         # Combine factors to adjust confidence
#         adjustment_factors = {
#             'symptom_coverage': symptom_count_factor,
#             'feature_importance': feature_importance_factor,
#             'case_similarity': case_similarity,
#             'symptom_severity': min(1.0, len(symptoms) / 5)  # More symptoms â†’ higher confidence
#         }
        
#         # Weighted combination
#         weights = [0.3, 0.25, 0.25, 0.2]  # Adjustable weights
#         adjustment = sum(w * adjustment_factors[list(adjustment_factors.keys())[i]] 
#                         for i, w in enumerate(weights))
        
#         # Apply adjustment to base confidence
#         ai_confidence = min(100, base_confidence * (1 + adjustment * 0.3))
        
#         return {
#             'ai_confidence': round(ai_confidence, 2),
#             'feature_importance': feature_importance,
#             'similar_cases': similar_cases,
#             'adjustment_factors': {k: round(v, 3) for k, v in adjustment_factors.items()}
#         }
    
#     def rerank_predictions(self, predictions: List[Dict], symptoms: List[str], 
#                           demographics: Dict = None) -> List[Dict]:
#         """
#         Rerank disease predictions using AI scoring
#         """
#         if not predictions:
#             return predictions
        
#         # Enhance each prediction with AI scoring
#         for prediction in predictions:
#             ai_result = self.predict_ai_confidence(
#                 symptoms, 
#                 prediction['disease'], 
#                 prediction['confidence'],
#                 prediction['matched_symptoms']
#             )
            
#             prediction['ai_confidence'] = ai_result['ai_confidence']
#             prediction['ai_insights'] = {
#                 'feature_importance': ai_result['feature_importance'],
#                 'similar_cases': ai_result['similar_cases'],
#                 'adjustment_factors': ai_result['adjustment_factors']
#             }
        
#         # Sort by AI confidence instead of original confidence
#         reranked = sorted(predictions, key=lambda x: x['ai_confidence'], reverse=True)
        
#         return reranked

# class MultiSourceDataIntegrator:
#     """
#     Integrates data from multiple external sources for disease enrichment
#     """
    
#     def __init__(self, max_workers: int = 5, timeout: int = 10):
#         self.max_workers = max_workers
#         self.timeout = timeout
#         self.cache = {}
#         self.cache_ttl = 3600
        
#         logger.info("MultiSourceDataIntegrator initialized with %d workers", max_workers)
    
#     def _get_cached_data(self, key: str) -> Optional[Any]:
#         """Get data from cache if not expired"""
#         if key in self.cache:
#             data, timestamp = self.cache[key]
#             if time.time() - timestamp < self.cache_ttl:
#                 return data
#         return None
    
#     def _set_cached_data(self, key: str, data: Any) -> None:
#         """Store data in cache with timestamp"""
#         self.cache[key] = (data, time.time())
    
#     def _enrich_icd11_data(self, disease_name: str) -> Optional[Dict[str, Any]]:
#         """Enrich with WHO ICD-11 data (placeholder)"""
#         cache_key = f"icd11_{disease_name.lower()}"
#         cached_data = self._get_cached_data(cache_key)
#         if cached_data:
#             return cached_data
        
#         try:
#             icd11_data = {
#                 "icd11_code": f"ICD11_{hash(disease_name) % 10000:04d}",
#                 "disease_name": disease_name,
#                 "category": "Simulated Category",
#                 "chapter": "Simulated Chapter",
#                 "definition": f"ICD-11 classification for {disease_name} based on symptom patterns",
#                 "inclusion_terms": [f"Clinical variant {i}" for i in range(2)],
#                 "exclusion_terms": ["Other specified conditions"],
#                 "severity_classification": "Moderate",
#                 "public_health_importance": "Medium",
#                 "last_updated": datetime.now().isoformat()
#             }
            
#             self._set_cached_data(cache_key, icd11_data)
#             return icd11_data
            
#         except Exception as e:
#             logger.error("Error enriching ICD-11 data for %s: %s", disease_name, str(e))
#             return None
    
#     def _enrich_clinical_trials(self, disease_name: str) -> List[Dict[str, Any]]:
#         """Enrich with clinical trials data (placeholder)"""
#         cache_key = f"trials_{disease_name.lower()}"
#         cached_data = self._get_cached_data(cache_key)
#         if cached_data:
#             return cached_data
        
#         try:
#             trials = []
#             for i in range(2):
#                 trial = {
#                     "nct_id": f"NCT{hash(disease_name + str(i)) % 1000000:06d}",
#                     "title": f"Phase {i+1} Clinical Trial for {disease_name}",
#                     "phase": f"Phase {i+1}",
#                     "status": ["Recruiting", "Completed", "Active"][i % 3],
#                     "conditions": [disease_name, "Related Symptoms"],
#                     "interventions": ["Experimental Drug", "Standard Care"],
#                     "locations": ["Multi-center", "International Sites"],
#                     "start_date": (datetime.now() - timedelta(days=300-i*100)).strftime("%Y-%m-%d"),
#                     "completion_date": (datetime.now() + timedelta(days=200+i*50)).strftime("%Y-%m-%d"),
#                     "sponsor": f"Medical Research Institute {i+1}",
#                     "brief_summary": f"Investigating novel treatments for {disease_name} based on recent findings"
#                 }
#                 trials.append(trial)
            
#             self._set_cached_data(cache_key, trials)
#             return trials
            
#         except Exception as e:
#             logger.error("Error enriching clinical trials for %s: %s", disease_name, str(e))
#             return []
    
#     def _enrich_drug_interactions(self, disease_name: str) -> List[Dict[str, Any]]:
#         """Enrich with drug interactions data (placeholder)"""
#         cache_key = f"drugs_{disease_name.lower()}"
#         cached_data = self._get_cached_data(cache_key)
#         if cached_data:
#             return cached_data
        
#         try:
#             drugs = []
#             drug_names = ["Analgesic Compound", "Anti-inflammatory", "Symptom Relief Formula"]
            
#             for i, drug_name in enumerate(drug_names):
#                 drug = {
#                     "drug_id": f"DB{hash(disease_name + drug_name) % 10000:05d}",
#                     "name": drug_name,
#                     "type": "Small Molecule",
#                     "groups": ["approved", "prescription"],
#                     "indications": [f"Management of {disease_name} symptoms"],
#                     "mechanism_of_action": f"Targets symptom pathways in {disease_name}",
#                     "interactions": [
#                         {
#                             "interacting_drug": "Common Medication",
#                             "description": "Monitor for additive effects",
#                             "severity": "Moderate"
#                         }
#                     ],
#                     "side_effects": ["Mild discomfort", "Temporary symptoms"],
#                     "dosage": "As clinically indicated",
#                     "contraindications": ["Severe conditions", "Specific allergies"]
#                 }
#                 drugs.append(drug)
            
#             self._set_cached_data(cache_key, drugs)
#             return drugs
            
#         except Exception as e:
#             logger.error("Error enriching drug interactions for %s: %s", disease_name, str(e))
#             return []
    
#     def _enrich_genetic_markers(self, disease_name: str) -> List[Dict[str, Any]]:
#         """Enrich with genetic markers data (placeholder)"""
#         cache_key = f"genetic_{disease_name.lower()}"
#         cached_data = self._get_cached_data(cache_key)
#         if cached_data:
#             return cached_data
        
#         try:
#             markers = []
#             for i in range(2):
#                 marker = {
#                     "rs_id": f"rs{hash(disease_name + str(i)) % 1000000}",
#                     "chromosome": f"Chr{(i % 22) + 1}",
#                     "position": f"{1000000 + i * 50000}",
#                     "gene": f"GENE_{disease_name.upper().replace(' ', '_')}_{i}",
#                     "risk_allele": f"Variant_{i}",
#                     "odds_ratio": round(1.1 + i * 0.2, 2),
#                     "p_value": f"5e-{6 + i}",
#                     "population": "Mixed Ancestry",
#                     "significance": "Suggestive",
#                     "study_reference": f"GWAS meta-analysis for {disease_name}"
#                 }
#                 markers.append(marker)
            
#             self._set_cached_data(cache_key, markers)
#             return markers
            
#         except Exception as e:
#             logger.error("Error enriching genetic markers for %s: %s", disease_name, str(e))
#             return []
    
#     def _enrich_environmental_factors(self, disease_name: str, location: str = None) -> Optional[Dict[str, Any]]:
#         """Enrich with environmental factors data (placeholder)"""
#         cache_key = f"env_{disease_name.lower()}_{location or 'global'}"
#         cached_data = self._get_cached_data(cache_key)
#         if cached_data:
#             return cached_data
        
#         try:
#             env_data = {
#                 "location": location or "Global",
#                 "air_quality_index": 42,
#                 "pollen_level": "Low-Moderate",
#                 "temperature": 21.5,
#                 "humidity": 60,
#                 "uv_index": 3,
#                 "precipitation": 0.1,
#                 "seasonal_factors": ["Seasonal variation observed"],
#                 "risk_factors": [
#                     "Environmental triggers may exacerbate symptoms",
#                     "Consider indoor air quality"
#                 ],
#                 "recommendations": [
#                     "Monitor local environmental conditions",
#                     "Take precautions during high-risk periods"
#                 ],
#                 "last_updated": datetime.now().isoformat()
#             }
            
#             self._set_cached_data(cache_key, env_data)
#             return env_data
            
#         except Exception as e:
#             logger.error("Error enriching environmental factors for %s: %s", disease_name, str(e))
#             return None
    
#     def _enrich_patient_insights(self, disease_name: str) -> List[Dict[str, Any]]:
#         """Enrich with patient insights data (placeholder)"""
#         cache_key = f"patient_{disease_name.lower()}"
#         cached_data = self._get_cached_data(cache_key)
#         if cached_data:
#             return cached_data
        
#         try:
#             insights = []
#             sources = ["Patient Community", "Health Forum", "Support Group"]
            
#             for i, source in enumerate(sources):
#                 insight = {
#                     "source": source,
#                     "title": f"Living with {disease_name} - Patient Perspectives",
#                     "content": f"Community discussions highlight common experiences with {disease_name} management",
#                     "sentiment": ["positive", "neutral", "mixed"][i % 3],
#                     "common_themes": [
#                         "Symptom management strategies",
#                         "Treatment experiences",
#                         "Lifestyle adaptations"
#                     ],
#                     "helpful_tips": [
#                         "Keep a symptom journal",
#                         "Communicate with healthcare providers"
#                     ],
#                     "date_posted": (datetime.now() - timedelta(days=i*30)).strftime("%Y-%m-%d"),
#                     "relevance_score": round(0.85 - i * 0.1, 2)
#                 }
#                 insights.append(insight)
            
#             self._set_cached_data(cache_key, insights)
#             return insights
            
#         except Exception as e:
#             logger.error("Error enriching patient insights for %s: %s", disease_name, str(e))
#             return []
    
#     def enrich_disease_data(self, disease_name: str, location: str = None) -> EnrichmentData:
#         """Main method to enrich disease data from all sources"""
#         logger.info("Enriching data for disease: %s", disease_name)
#         start_time = time.time()
        
#         enrichment_data = EnrichmentData()
        
#         # Parallel API calls
#         with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
#             futures = {
#                 executor.submit(self._enrich_icd11_data, disease_name): 'icd11_data',
#                 executor.submit(self._enrich_clinical_trials, disease_name): 'clinical_trials',
#                 executor.submit(self._enrich_drug_interactions, disease_name): 'drug_interactions',
#                 executor.submit(self._enrich_genetic_markers, disease_name): 'genetic_markers',
#                 executor.submit(self._enrich_environmental_factors, disease_name, location): 'environmental_factors',
#                 executor.submit(self._enrich_patient_insights, disease_name): 'patient_insights'
#             }
            
#             for future in as_completed(futures):
#                 try:
#                     result = future.result()
#                     setattr(enrichment_data, futures[future], result)
#                 except Exception as e:
#                     logger.warning("Failed to get enrichment result: %s", str(e))
        
#         elapsed_time = time.time() - start_time
#         logger.info("Completed enrichment for %s in %.2f seconds", disease_name, elapsed_time)
        
#         return enrichment_data

# class SymptomMatcher:
#     """Handles symptom normalization, matching, and severity weighting"""
    
#     def __init__(self):
#         self.symptom_severity = {}
#         self.canonical_symptoms = set()
#         self.symptom_mapping = {}
#         self._load_severity_data()
        
#     def _load_severity_data(self) -> None:
#         """Load symptom severity data"""
#         try:
#             # Create mock severity data
#             sample_symptoms = {
#                 'fever': 3, 'cough': 2, 'headache': 2, 'fatigue': 1, 'nausea': 2,
#                 'chest_pain': 4, 'shortness_of_breath': 4, 'dizziness': 2, 'rash': 2,
#                 'abdominal_pain': 3, 'vomiting': 3, 'diarrhea': 2, 'constipation': 1
#             }
            
#             for symptom, weight in sample_symptoms.items():
#                 normalized = self._normalize_name(symptom)
#                 self.symptom_severity[normalized] = weight
#                 self.canonical_symptoms.add(normalized)
                
#             logger.info(f"Loaded {len(self.symptom_severity)} symptoms with severity data")
            
#         except Exception as e:
#             logger.error(f"Error loading symptom severity data: {e}")
#             # Create minimal fallback
#             self.symptom_severity = {'fever': 2, 'cough': 1, 'headache': 1}
#             self.canonical_symptoms = set(self.symptom_severity.keys())
            
#     def _normalize_name(self, name: str) -> str:
#         """Normalize symptom/disease names"""
#         if not isinstance(name, str) or pd.isna(name):
#             return ""
        
#         name = name.lower().strip()
#         name = re.sub(r'[^a-z0-9\s]', '', name)
#         name = re.sub(r'\s+', '_', name)
#         return name
    
#     def add_symptom(self, symptom: str) -> str:
#         """Add symptom to canonical list"""
#         normalized = self._normalize_name(symptom)
#         if normalized and normalized not in self.canonical_symptoms:
#             self.canonical_symptoms.add(normalized)
#             if normalized not in self.symptom_severity:
#                 self.symptom_severity[normalized] = 1
#         return normalized
        
#     def map_to_canonical(self, symptom: str) -> Optional[str]:
#         """Map input symptom to canonical form using fuzzy matching"""
#         if not symptom:
#             return None
            
#         normalized = self._normalize_name(symptom)
        
#         # Exact match
#         if normalized in self.canonical_symptoms:
#             return normalized
            
#         # Fuzzy match
#         if self.canonical_symptoms:
#             match, score, _ = process.extractOne(
#                 normalized, 
#                 list(self.canonical_symptoms), 
#                 scorer=fuzz.token_sort_ratio
#             )
            
#             if score >= FUZZY_MATCH_THRESHOLD:
#                 return match
                
#         return None
        
#     def get_severity(self, symptom: str) -> int:
#         """Get severity weight for a symptom"""
#         return self.symptom_severity.get(symptom, 1)

# class DiseaseDatabase:
#     """Manages disease data including symptoms, descriptions, and precautions"""
    
#     def __init__(self, symptom_matcher: SymptomMatcher):
#         self.symptom_matcher = symptom_matcher
#         self.diseases = {}
#         self._load_data()
        
#     def _load_data(self) -> None:
#         """Load and preprocess disease datasets"""
#         try:
#             # Create mock disease database
#             mock_diseases = {
#                 'influenza': {
#                     'symptoms': {'fever', 'cough', 'fatigue', 'headache', 'muscle_aches'},
#                     'description': 'Viral infection affecting respiratory system',
#                     'precautions': ['Rest', 'Hydration', 'Fever reducers', 'Medical consultation']
#                 },
#                 'common_cold': {
#                     'symptoms': {'cough', 'sore_throat', 'runny_nose', 'sneezing'},
#                     'description': 'Mild viral infection of upper respiratory tract',
#                     'precautions': ['Rest', 'Fluids', 'Over-the-counter cold medicine']
#                 },
#                 'migraine': {
#                     'symptoms': {'headache', 'nausea', 'sensitivity_to_light', 'dizziness'},
#                     'description': 'Neurological condition characterized by intense headaches',
#                     'precautions': ['Rest in dark room', 'Medication', 'Avoid triggers']
#                 },
#                 'gastroenteritis': {
#                     'symptoms': {'nausea', 'vomiting', 'diarrhea', 'abdominal_pain', 'fever'},
#                     'description': 'Inflammation of stomach and intestines',
#                     'precautions': ['Hydration', 'Bland diet', 'Medical attention if severe']
#                 }
#             }
            
#             for disease_name, data in mock_diseases.items():
#                 normalized_name = self.symptom_matcher._normalize_name(disease_name)
                
#                 # Ensure all symptoms are in the matcher
#                 for symptom in data['symptoms']:
#                     self.symptom_matcher.add_symptom(symptom)
                
#                 total_severity = sum(self.symptom_matcher.get_severity(s) for s in data['symptoms'])
                
#                 self.diseases[normalized_name] = {
#                     'symptoms': data['symptoms'],
#                     'total_severity': total_severity,
#                     'description': data['description'],
#                     'precautions': data['precautions']
#                 }
            
#             logger.info(f"Loaded {len(self.diseases)} diseases")
            
#         except Exception as e:
#             logger.error(f"Error loading disease data: {e}")
#             # Create minimal fallback
#             self.diseases = {
#                 'influenza': {
#                     'symptoms': {'fever', 'cough'},
#                     'total_severity': 3,
#                     'description': 'Sample disease',
#                     'precautions': ['Consult doctor']
#                 }
#             }
    
#     def get_all_symptoms(self) -> List[str]:
#         """Get all unique canonical symptoms"""
#         all_symptoms = set()
#         for disease_data in self.diseases.values():
#             all_symptoms.update(disease_data['symptoms'])
#         return sorted(list(all_symptoms))
    
#     def find_matching_diseases(self, user_symptoms: List[str]) -> List[Dict]:
#         """Find diseases matching provided symptoms with confidence scoring"""
#         results = []
#         user_symptoms_set = set(user_symptoms)
        
#         for disease, data in self.diseases.items():
#             disease_symptoms = data['symptoms']
#             matched_symptoms = user_symptoms_set.intersection(disease_symptoms)
            
#             if len(matched_symptoms) < MIN_MATCHED_SYMPTOMS:
#                 continue
                
#             matched_severity = sum(self.symptom_matcher.get_severity(s) for s in matched_symptoms)
#             total_severity = data['total_severity']
            
#             if total_severity == 0:
#                 continue
                
#             confidence = (matched_severity / total_severity) * 100
            
#             if confidence < MIN_CONFIDENCE_THRESHOLD:
#                 continue
                
#             unmatched_symptoms = disease_symptoms - user_symptoms_set
            
#             results.append({
#                 'disease': disease.replace('_', ' ').title(),
#                 'original_disease_name': disease,
#                 'confidence': round(confidence, 2),
#                 'matched_symptoms': [s.replace('_', ' ').title() for s in matched_symptoms],
#                 'unmatched_symptoms': [s.replace('_', ' ').title() for s in unmatched_symptoms],
#                 'description': data['description'],
#                 'precautions': data['precautions'],
#                 'severity_score': total_severity
#             })
        
#         results.sort(key=lambda x: (len(x['matched_symptoms']), x['confidence']), reverse=True)
#         return results[:TOP_N_DISEASES]

# # Initialize components
# try:
#     symptom_matcher = SymptomMatcher()
#     disease_db = DiseaseDatabase(symptom_matcher)
#     data_integrator = MultiSourceDataIntegrator(max_workers=5, timeout=15)
#     ai_engine = AIDiagnosticEngine()
#     logger.info("All API components initialized successfully")
    
# except Exception as e:
#     logger.error(f"Failed to initialize API components: {e}")
#     # Create minimal fallbacks
#     symptom_matcher = SymptomMatcher()
#     disease_db = DiseaseDatabase(symptom_matcher)
#     data_integrator = MultiSourceDataIntegrator()
#     ai_engine = AIDiagnosticEngine()

# @app.route("/predict", methods=["POST"])
# def predict():
#     """
#     Enhanced prediction endpoint with AI reranking and data enrichment
#     """
#     try:
#         data = request.get_json()
#         if not data or 'symptoms' not in data:
#             return jsonify({
#                 "success": False,
#                 "error": "Missing symptoms in request body"
#             }), 400
        
#         input_symptoms = data['symptoms']
#         location = data.get('location')
#         enable_enrichment = data.get('enable_enrichment', True)
#         enable_ai_reranking = data.get('enable_ai_reranking', True)
#         demographics = data.get('demographics', {})
        
#         # Validate input
#         if not isinstance(input_symptoms, list):
#             return jsonify({
#                 "success": False,
#                 "error": "Symptoms must be provided as a list"
#             }), 400
            
#         if len(input_symptoms) < MIN_SYMPTOMS_REQUIRED:
#             return jsonify({
#                 "success": False,
#                 "error": f"At least {MIN_SYMPTOMS_REQUIRED} symptom is required"
#             }), 400
            
#         if len(input_symptoms) > MAX_SYMPTOMS_ALLOWED:
#             return jsonify({
#                 "success": False,
#                 "error": f"Maximum {MAX_SYMPTOMS_ALLOWED} symptoms allowed"
#             }), 400
        
#         logger.info(f"Prediction request: {len(input_symptoms)} symptoms, AI: {enable_ai_reranking}")
        
#         # Normalize symptoms
#         canonical_symptoms = []
#         unrecognized_symptoms = []
#         for symptom in input_symptoms:
#             canonical = symptom_matcher.map_to_canonical(symptom)
#             if canonical:
#                 canonical_symptoms.append(canonical)
#             else:
#                 unrecognized_symptoms.append(symptom)
        
#         if unrecognized_symptoms:
#             suggestions = {}
#             for symptom in unrecognized_symptoms:
#                 normalized = symptom_matcher._normalize_name(symptom)
#                 if symptom_matcher.canonical_symptoms:
#                     matches = process.extract(
#                         normalized, 
#                         list(symptom_matcher.canonical_symptoms), 
#                         scorer=fuzz.token_sort_ratio,
#                         limit=3
#                     )
#                     suggestions[symptom] = [match[0] for match in matches if match[1] > 50]
            
#             return jsonify({
#                 "success": False,
#                 "error": f"Unrecognized symptoms: {unrecognized_symptoms}",
#                 "suggestions": suggestions
#             }), 400
        
#         # Find matching diseases
#         matched_diseases = disease_db.find_matching_diseases(canonical_symptoms)
        
#         # Apply AI reranking if enabled
#         if enable_ai_reranking and matched_diseases:
#             logger.info("Applying AI diagnostic engine reranking")
#             matched_diseases = ai_engine.rerank_predictions(
#                 matched_diseases, canonical_symptoms, demographics
#             )
        
#         # Enrich disease data if enabled
#         enriched_predictions = []
#         if enable_enrichment and matched_diseases:
#             logger.info(f"Enriching data for {len(matched_diseases)} diseases")
#             for disease_pred in matched_diseases:
#                 try:
#                     enriched_data = data_integrator.enrich_disease_data(
#                         disease_pred['original_disease_name'], location
#                     )
                    
#                     enriched_pred = disease_pred.copy()
#                     enriched_pred['external_data'] = {
#                         'icd11_classification': enriched_data.icd11_data,
#                         'clinical_trials': enriched_data.clinical_trials,
#                         'drug_interactions': enriched_data.drug_interactions,
#                         'genetic_markers': enriched_data.genetic_markers,
#                         'environmental_factors': enriched_data.environmental_factors,
#                         'patient_insights': enriched_data.patient_insights,
#                         'enrichment_timestamp': datetime.now().isoformat()
#                     }
#                     enriched_predictions.append(enriched_pred)
                    
#                 except Exception as e:
#                     logger.error(f"Enrichment error for {disease_pred['disease']}: {e}")
#                     disease_pred['external_data'] = {'enrichment_error': str(e)}
#                     enriched_predictions.append(disease_pred)
#         else:
#             enriched_predictions = matched_diseases
        
#         # Calculate health score
#         total_severity = sum(symptom_matcher.get_severity(s) for s in canonical_symptoms)
#         avg_severity = total_severity / len(canonical_symptoms) if canonical_symptoms else 0
#         health_score = max(0, 100 - min(100, avg_severity * 10))
        
#         response = {
#             "input_symptoms": [s.replace('_', ' ').title() for s in canonical_symptoms],
#             "health_score": round(health_score, 2),
#             "predictions": enriched_predictions,
#             "data_enrichment_enabled": enable_enrichment,
#             "ai_reranking_enabled": enable_ai_reranking,
#             "recommendation": "Consult healthcare professional for accurate diagnosis"
#         }
        
#         return jsonify({
#             "success": True, 
#             "data": response, 
#             "meta": {
#                 "predictions_found": len(matched_diseases),
#                 "symptoms_provided": len(input_symptoms),
#                 "symptoms_matched": len(canonical_symptoms),
#                 "ai_enhanced": enable_ai_reranking,
#                 "response_timestamp": datetime.now().isoformat()
#             }
#         })
        
#     except Exception as e:
#         logger.error(f"Prediction error: {e}", exc_info=True)
#         return jsonify({
#             "success": False,
#             "error": "Internal server error during prediction"
#         }), 500

# @app.route("/symptoms", methods=["GET"])
# def get_symptoms():
#     """Get all known symptoms"""
#     try:
#         symptoms = disease_db.get_all_symptoms()
#         formatted_symptoms = [s.replace('_', ' ').title() for s in symptoms]
#         return jsonify({"success": True, "data": formatted_symptoms})
#     except Exception as e:
#         logger.error(f"Error fetching symptoms: {e}")
#         return jsonify({"success": False, "error": "Failed to retrieve symptoms"}), 500

# @app.route("/diseases", methods=["GET"])
# def get_diseases():
#     """Get all known diseases"""
#     try:
#         diseases = list(disease_db.diseases.keys())
#         formatted_diseases = [d.replace('_', ' ').title() for d in diseases]
#         return jsonify({"success": True, "data": formatted_diseases})
#     except Exception as e:
#         logger.error(f"Error fetching diseases: {e}")
#         return jsonify({"success": False, "error": "Failed to retrieve diseases"}), 500

# @app.route("/symptom_severity/<symptom>", methods=["GET"])
# def get_symptom_severity(symptom):
#     """Get severity of a specific symptom"""
#     try:
#         canonical_symptom = symptom_matcher.map_to_canonical(symptom)
#         if not canonical_symptom:
#             return jsonify({"success": False, "error": "Symptom not found"}), 404
            
#         severity = symptom_matcher.get_severity(canonical_symptom)
#         return jsonify({
#             "success": True, 
#             "symptom": canonical_symptom.replace('_', ' ').title(), 
#             "severity": severity
#         })
#     except Exception as e:
#         logger.error(f"Error fetching symptom severity: {e}")
#         return jsonify({"success": False, "error": "Failed to retrieve symptom severity"}), 500

# @app.route("/enrich_disease/<disease_name>", methods=["GET"])
# def enrich_single_disease(disease_name: str):
#     """Enrich a single disease with external data"""
#     try:
#         if not disease_name.strip():
#             return jsonify({"success": False, "error": "Disease name required"}), 400
        
#         location = request.args.get('location')
#         enriched_data = data_integrator.enrich_disease_data(disease_name, location)
        
#         return jsonify({
#             "success": True,
#             "disease": disease_name,
#             "enriched_data": {
#                 'icd11_classification': enriched_data.icd11_data,
#                 'clinical_trials': enriched_data.clinical_trials,
#                 'drug_interactions': enriched_data.drug_interactions,
#                 'genetic_markers': enriched_data.genetic_markers,
#                 'environmental_factors': enriched_data.environmental_factors,
#                 'patient_insights': enriched_data.patient_insights
#             },
#             "meta": {
#                 "enrichment_timestamp": datetime.now().isoformat()
#             }
#         })
        
#     except Exception as e:
#         logger.error(f"Error enriching disease {disease_name}: {e}")
#         return jsonify({"success": False, "error": f"Failed to enrich disease: {str(e)}"}), 500

# @app.route("/ai_analysis", methods=["POST"])
# def ai_analysis():
#     """Standalone endpoint for AI analysis of symptoms"""
#     try:
#         data = request.get_json()
#         symptoms = data.get('symptoms', [])
#         disease = data.get('disease', '')
        
#         if not symptoms or not disease:
#             return jsonify({"success": False, "error": "Symptoms and disease required"}), 400
        
#         analysis = ai_engine.predict_ai_confidence(symptoms, disease, 50.0, symptoms)
        
#         return jsonify({
#             "success": True,
#             "analysis": analysis,
#             "meta": {
#                 "symptoms_analyzed": len(symptoms),
#                 "analysis_timestamp": datetime.now().isoformat()
#             }
#         })
        
#     except Exception as e:
#         logger.error(f"AI analysis error: {e}")
#         return jsonify({"success": False, "error": "AI analysis failed"}), 500

# @app.route("/health", methods=["GET"])
# def health_check():
#     """Health check endpoint"""
#     health_status = {
#         "status": "healthy",
#         "diseases_loaded": len(disease_db.diseases),
#         "symptoms_loaded": len(symptom_matcher.canonical_symptoms),
#         "ai_engine": "active",
#         "data_integrator": "active",
#         "timestamp": datetime.now().isoformat()
#     }
    
#     return jsonify({"success": True, "status": health_status})

# @app.errorhandler(404)
# def not_found(e):
#     return jsonify({"success": False, "error": "Endpoint not found"}), 404

# @app.errorhandler(500)
# def server_error(e):
#     logger.error(f"Server error: {e}")
#     return jsonify({"success": False, "error": "Internal server error"}), 500

# if __name__ == "__main__":
#     app.run(debug=True, host="0.0.0.0", port=5000)

# Add these imports at the top
#!/usr/bin/env python3
"""
Production-Grade Medical Diagnostic AI Backend
Google DeepMind Research-to-Production Deployment Standard
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import json
import hashlib

import torch
import torch.nn as nn
import torch.jit
from transformers import AutoTokenizer, AutoModel
import xgboost as xgb
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
import redis
from pydantic import BaseModel, Field, validator
import psutil
# import GPUtil  # Removed because not used and causes import error
from contextlib import asynccontextmanager
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge
import uuid
from concurrent.futures import ThreadPoolExecutor

# Configure structured JSON logging
logging.basicConfig(
    level=logging.INFO,
    format='{"timestamp": "%(asctime)s", "service": "%(name)s", "level": "%(levelname)s", "message": "%(message)s", "module": "%(module)s", "line": "%(lineno)d"}',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('medical_ai_production.log')
    ]
)
logger = logging.getLogger("medical-ai-backend")

# Prometheus metrics
REQUEST_COUNT = Counter('request_total', 'Total API requests', ['method', 'endpoint', 'status'])
REQUEST_LATENCY = Histogram('request_latency_seconds', 'Request latency', ['endpoint'])
GPU_MEMORY = Gauge('gpu_memory_usage_bytes', 'GPU memory usage')
CPU_USAGE = Gauge('cpu_usage_percent', 'CPU usage percentage')
MEMORY_USAGE = Gauge('memory_usage_bytes', 'Memory usage in bytes')
CACHE_HITS = Counter('cache_hits_total', 'Cache hit counter', ['cache_type'])
CACHE_MISSES = Counter('cache_misses_total', 'Cache miss counter', ['cache_type'])

class ProductionConfig:
    """Production configuration with environment-based defaults"""
    BERT_MODEL = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
    BATCH_SIZE = 64
    MAX_SEQ_LENGTH = 512
    CACHE_TTL = 7200  # 2 hours
    SIMILARITY_THRESHOLD = 0.25
    GPU_PRIORITY = True
    MODEL_VERSION = "v3.0.0-production"
    REDIS_URL = "redis://localhost:6379/0"
    MAX_WORKERS = 4
    REQUEST_TIMEOUT = 30.0
    HEALTH_CHECK_INTERVAL = 30

# Pydantic models with comprehensive validation
class SymptomRequest(BaseModel):
    symptoms: List[str] = Field(..., min_items=1, max_items=50, description="List of patient symptoms")
    demographics: Dict[str, Any] = Field(default_factory=dict, description="Patient demographic information")
    patient_history: Dict[str, Any] = Field(default_factory=dict, description="Medical history and timeline")
    
    @validator('symptoms')
    def validate_symptoms(cls, v):
        if len(v) > 50:
            raise ValueError('Maximum 50 symptoms allowed')
        return [symptom.strip().lower() for symptom in v if symptom.strip()]

class TemporalRequest(BaseModel):
    symptom_timeline: List[Dict[str, Any]] = Field(..., min_items=2, description="Timeline of symptom progression")
    patient_id: Optional[str] = Field(None, description="Unique patient identifier")

class SimilarPatientsRequest(BaseModel):
    symptoms: List[str] = Field(..., min_items=1)
    demographics: Dict[str, Any] = Field(default_factory=dict)
    k: int = Field(5, ge=1, le=50)

class AnalysisResponse(BaseModel):
    success: bool
    analysis_id: str
    results: Dict[str, Any]
    metadata: Dict[str, Any]
    warnings: List[str] = []

@dataclass
class PerformanceMetrics:
    inference_time: float
    cache_hit: bool
    model_version: str
    device_used: str
    memory_usage: float

class SecureDeviceManager:
    """Advanced device management with GPU optimization and memory monitoring"""
    
    def __init__(self, gpu_priority: bool = True):
        self.gpu_priority = gpu_priority
        self.device = self._initialize_device()
        self.memory_monitor = MemoryMonitor()
        logger.info(f"Device manager initialized on {self.device}")
    
    def _initialize_device(self) -> torch.device:
        if self.gpu_priority and torch.cuda.is_available():
            device = torch.device('cuda')
            torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes
            torch.set_float32_matmul_precision('high')  # Use TF32 on Ampere+
            logger.info("CUDA optimized backend enabled")
        elif self.gpu_priority and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device('mps')
            logger.info("MPS (Apple Silicon) backend enabled")
        else:
            device = torch.device('cpu')
            logger.info("Using CPU backend")
        
        return device
    
    def optimize_model(self, model: nn.Module) -> nn.Module:
        """Apply device-specific optimizations"""
        model = model.to(self.device)
        
        if self.device.type == 'cuda':
            # Enable mixed precision for modern GPUs
            if torch.cuda.get_device_capability()[0] >= 7:  # Ampere or newer
                model = model.half()  # Use FP16
                logger.info("Mixed precision (FP16) enabled for GPU")
        
        model.eval()
        return model
    
    def get_device_status(self) -> Dict[str, Any]:
        """Comprehensive device status report"""
        status = {
            "device_type": self.device.type,
            "gpu_available": torch.cuda.is_available(),
            "memory_usage": self.memory_monitor.get_current_usage()
        }
        
        if self.device.type == 'cuda':
            status.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory,
                "gpu_memory_allocated": torch.cuda.memory_allocated(0),
                "gpu_memory_reserved": torch.cuda.memory_reserved(0)
            })
        
        return status

class MemoryMonitor:
    """Real-time memory usage monitoring"""
    
    def get_current_usage(self) -> Dict[str, float]:
        process = psutil.Process()
        memory_info = process.memory_info()
        
        usage = {
            "process_rss_mb": memory_info.rss / 1024 / 1024,
            "process_vms_mb": memory_info.vms / 1024 / 1024,
            "system_available_percent": psutil.virtual_memory().percent
        }
        
        if torch.cuda.is_available():
            usage.update({
                "gpu_allocated_mb": torch.cuda.memory_allocated(0) / 1024 / 1024,
                "gpu_reserved_mb": torch.cuda.memory_reserved(0) / 1024 / 1024
            })
        
        return usage

class ProductionCacheManager:
    """High-performance caching with Redis cluster support and fallback strategies"""
    
    def __init__(self, redis_url: str = ProductionConfig.REDIS_URL):
        self.redis_client = None
        self.local_cache = {}
        self.local_cache_ttl = {}
        self.redis_available = False
        
        try:
            self.redis_client = redis.Redis.from_url(
                redis_url, 
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
                retry_on_timeout=True,
                max_connections=20
            )
            # Test connection
            self.redis_client.ping()
            self.redis_available = True
            logger.info("Redis cluster connection established")
        except Exception as e:
            logger.warning(f"Redis unavailable, using local cache: {e}")
            self.redis_client = None
    
    def generate_key(self, prefix: str, data: Any) -> str:
        """Generate deterministic cache key with namespace"""
        data_str = json.dumps(data, sort_keys=True, separators=(',', ':'))
        hash_digest = hashlib.sha256(data_str.encode()).hexdigest()[:16]
        return f"medai:{prefix}:{hash_digest}"
    
    async def get(self, key: str) -> Optional[Any]:
        """Async cache get with fallback"""
        # Check local cache first
        if key in self.local_cache:
            if time.time() - self.local_cache_ttl.get(key, 0) < 300:  # 5min TTL for local
                CACHE_HITS.labels(cache_type='local').inc()
                return self.local_cache[key]
            else:
                del self.local_cache[key]
                del self.local_cache_ttl[key]
        
        # Try Redis
        if self.redis_available:
            try:
                result = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: self.redis_client.get(key)
                )
                if result:
                    CACHE_HITS.labels(cache_type='redis').inc()
                    parsed = json.loads(result)
                    # Populate local cache
                    self.local_cache[key] = parsed
                    self.local_cache_ttl[key] = time.time()
                    return parsed
                CACHE_MISSES.labels(cache_type='redis').inc()
            except Exception as e:
                logger.error(f"Redis get error: {e}")
                self.redis_available = False
        
        CACHE_MISSES.labels(cache_type='local').inc()
        return None
    
    async def set(self, key: str, value: Any, ttl: int = ProductionConfig.CACHE_TTL):
        """Async cache set with fallback"""
        # Always set in local cache
        self.local_cache[key] = value
        self.local_cache_ttl[key] = time.time()
        
        # Set in Redis if available
        if self.redis_available:
            try:
                await asyncio.get_event_loop().run_in_executor(
                    None, 
                    lambda: self.redis_client.setex(
                        key, ttl, json.dumps(value, separators=(',', ':'))
                    )
                )
            except Exception as e:
                logger.error(f"Redis set error: {e}")
                self.redis_available = False

class OptimizedBERTEncoder:
    """Production-optimized BERT encoder with ONNX support and batch optimization"""
    
    def __init__(self, model_name: str = ProductionConfig.BERT_MODEL):
        self.device_manager = SecureDeviceManager(ProductionConfig.GPU_PRIORITY)
        self.cache = ProductionCacheManager()
        self.batch_executor = ThreadPoolExecutor(max_workers=2)
        
        # Load with progress tracking
        logger.info(f"Loading BERT model: {model_name}")
        start_time = time.time()
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            use_fast=True,  # Fast tokenizer for better performance
            truncation_side='right'
        )
        
        self.model = AutoModel.from_pretrained(model_name)
        self.model = self.device_manager.optimize_model(self.model)
        
        # Add padding token if not present
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token or '[PAD]'
        
        load_time = time.time() - start_time
        logger.info(f"BERT model loaded in {load_time:.2f}s on {self.device_manager.device}")
    
    async def encode_batch_async(self, symptoms_batch: List[List[str]]) -> np.ndarray:
        """Async batch encoding with optimal resource usage"""
        if not symptoms_batch:
            return np.zeros((0, 768))
        
        cache_key = self.cache.generate_key("bert_batch", symptoms_batch)
        cached = await self.cache.get(cache_key)
        if cached is not None:
            return np.array(cached)
        
        # Process in parallel batches
        loop = asyncio.get_event_loop()
        embeddings = await loop.run_in_executor(
            self.batch_executor,
            self._encode_batch_sync,
            symptoms_batch
        )
        
        await self.cache.set(cache_key, embeddings.tolist())
        return embeddings
    
    def _encode_batch_sync(self, symptoms_batch: List[List[str]]) -> np.ndarray:
        """Synchronous batch processing for executor"""
        texts = [". ".join(symptoms) for symptoms in symptoms_batch]
        all_embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), ProductionConfig.BATCH_SIZE):
                batch_texts = texts[i:i + ProductionConfig.BATCH_SIZE]
                
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=ProductionConfig.MAX_SEQ_LENGTH,
                    return_tensors="pt",
                    return_attention_mask=True
                )
                
                inputs = {k: v.to(self.device_manager.device) for k, v in inputs.items()}
                
                outputs = self.model(**inputs)
                # Use mean pooling with attention mask
                attention_mask = inputs['attention_mask']
                token_embeddings = outputs.last_hidden_state
                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()
                
                all_embeddings.append(batch_embeddings)
        
        return np.vstack(all_embeddings)

class TemporalAnalysisEngine:
    """Advanced temporal analysis with multi-modal sequence modeling"""
    
    def __init__(self, hidden_size: int = 128):
        self.device_manager = SecureDeviceManager(ProductionConfig.GPU_PRIORITY)
        self.cache = ProductionCacheManager()
        self.hidden_size = hidden_size
        
        # Initialize optimized LSTM model
        self.lstm_model = self._build_temporal_model()
        logger.info("Temporal analysis engine initialized")
    
    def _build_temporal_model(self) -> nn.Module:
        """Build optimized temporal model"""
        class TemporalLSTM(nn.Module):
            def __init__(self, input_size: int, hidden_size: int, num_layers: int = 2):
                super().__init__()
                self.lstm = nn.LSTM(
                    input_size, hidden_size, num_layers,
                    batch_first=True, bidirectional=True, dropout=0.2
                )
                self.attention = nn.MultiheadAttention(hidden_size * 2, num_heads=4, batch_first=True)
                self.classifier = nn.Sequential(
                    nn.Linear(hidden_size * 2, 64),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(64, 3)  # [improving, stable, worsening]
                )
            
            def forward(self, x: torch.Tensor) -> torch.Tensor:
                lstm_out, _ = self.lstm(x)
                attended, _ = self.attention(lstm_out, lstm_out, lstm_out)
                # Use last hidden state for classification
                last_hidden = attended[:, -1, :]
                return self.classifier(last_hidden)
        
        model = TemporalLSTM(input_size=50, hidden_size=self.hidden_size)
        return self.device_manager.optimize_model(model)
    
    async def analyze_temporal_patterns(self, sequences: List[List[Dict]]) -> List[Dict[str, Any]]:
        """Analyze temporal patterns with clinical interpretation"""
        results = []
        
        for sequence in sequences:
            cache_key = self.cache.generate_key("temporal", sequence)
            cached = await self.cache.get(cache_key)
            if cached:
                results.append(cached)
                continue
                
            analysis = await self._analyze_single_sequence(sequence)
            results.append(analysis)
            await self.cache.set(cache_key, analysis)
        
        return results
    
    async def _analyze_single_sequence(self, sequence: List[Dict]) -> Dict[str, Any]:
        """Analyze single symptom sequence"""
        if len(sequence) < 2:
            return {
                "trend": "insufficient_data",
                "urgency": "low",
                "confidence": 0.0,
                "recommendations": ["Collect more temporal data"]
            }
        
        try:
            # Extract features from sequence
            features = self._extract_temporal_features(sequence)
            
            # Rule-based analysis as fallback
            rule_based_result = self._rule_based_analysis(sequence)
            
            # If model is trained, use it (placeholder for model inference)
            model_result = rule_based_result  # In production, replace with model inference
            
            return {
                **model_result,
                "analysis_type": "temporal_pattern",
                "data_points": len(sequence),
                "time_span_days": self._calculate_time_span(sequence)
            }
            
        except Exception as e:
            logger.error(f"Temporal analysis error: {e}")
            return {
                "trend": "analysis_error",
                "urgency": "unknown",
                "confidence": 0.0,
                "error": str(e)
            }
    
    def _extract_temporal_features(self, sequence: List[Dict]) -> np.ndarray:
        """Extract comprehensive temporal features"""
        severities = np.array([s.get('severity', 0.0) for s in sequence])
        durations = np.array([s.get('duration_hours', 0.0) for s in sequence])
        
        features = []
        if len(severities) > 1:
            # Trend features
            features.extend([
                np.polyfit(range(len(severities)), severities, 1)[0],  # Slope
                np.std(severities),  # Volatility
                np.max(severities) - np.min(severities),  # Range
            ])
        
        return np.array(features)
    
    def _rule_based_analysis(self, sequence: List[Dict]) -> Dict[str, Any]:
        """Rule-based temporal analysis as fallback"""
        severities = [s.get('severity', 0.0) for s in sequence]
        
        if len(severities) < 2:
            return {"trend": "stable", "urgency": "low", "confidence": 0.5}
        
        # Calculate trend using linear regression
        x = np.arange(len(severities))
        slope = np.polyfit(x, severities, 1)[0]
        
        if slope > 0.3:
            trend, urgency = "worsening_rapidly", "high"
        elif slope > 0.1:
            trend, urgency = "worsening", "medium"
        elif slope < -0.1:
            trend, urgency = "improving", "low"
        else:
            trend, urgency = "stable", "low"
        
        return {
            "trend": trend,
            "urgency": urgency,
            "confidence": min(0.9, abs(slope) * 2),
            "recommendations": self._generate_temporal_recommendations(trend)
        }
    
    def _generate_temporal_recommendations(self, trend: str) -> List[str]:
        """Generate clinical recommendations based on trend"""
        recommendations = {
            "worsening_rapidly": [
                "Seek immediate medical attention",
                "Monitor symptoms closely",
                "Consider emergency care if severe"
            ],
            "worsening": [
                "Schedule doctor appointment within 24-48 hours",
                "Continue current medication",
                "Watch for warning signs"
            ],
            "stable": [
                "Maintain current treatment plan",
                "Schedule follow-up as needed",
                "Monitor for changes"
            ],
            "improving": [
                "Continue current treatment",
                "Gradually reduce medication as directed",
                "Maintain healthy habits"
            ]
        }
        return recommendations.get(trend, ["Consult healthcare provider"])

class XGBoostEnsemble:
    """Production XGBoost ensemble with GPU acceleration and interpretability"""
    
    def __init__(self):
        self.cache = ProductionCacheManager()
        self.models = {}
        self.feature_names = []
        self.class_names = []
        self._initialize_ensemble()
    
    def _initialize_ensemble(self):
        """Initialize with pre-trained models (placeholder for actual model loading)"""
        # In production, this would load actual trained models
        self.class_names = ['condition_a', 'condition_b', 'condition_c', 'healthy']
        logger.info("XGBoost ensemble initialized with placeholder models")
    
    async def predict_proba_batch(self, features_batch: List[Dict]) -> List[Dict[str, float]]:
        """Batch prediction with confidence scores"""
        if not self.models:
            return await self._fallback_prediction(features_batch)
        
        cache_key = self.cache.generate_key("xgboost_batch", features_batch)
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        # Convert features to numpy array
        X = self._features_to_array(features_batch)
        
        # Placeholder for actual prediction logic
        predictions = await self._dummy_prediction(X)
        
        await self.cache.set(cache_key, predictions)
        return predictions
    
    async def _dummy_prediction(self, X: np.ndarray) -> List[Dict[str, float]]:
        """Placeholder for actual model predictions"""
        # Simulate prediction time
        await asyncio.sleep(0.01)
        
        # Generate realistic-looking dummy predictions
        rng = np.random.default_rng(42)
        predictions = []
        
        for i in range(X.shape[0]):
            probs = rng.dirichlet(np.ones(len(self.class_names)))
            pred_dict = {cls: float(prob) for cls, prob in zip(self.class_names, probs)}
            predictions.append(pred_dict)
        
        return predictions
    
    async def _fallback_prediction(self, features_batch: List[Dict]) -> List[Dict[str, float]]:
        """Fallback when models aren't loaded"""
        return [{"unknown": 1.0} for _ in features_batch]
    
    def _features_to_array(self, features_batch: List[Dict]) -> np.ndarray:
        """Convert feature dicts to numpy array"""
        # Simple feature extraction (in production, this would be comprehensive)
        if not features_batch:
            return np.zeros((0, 10))
        
        feature_vectors = []
        for features in features_batch:
            vector = [
                features.get('age', 0) / 100.0,
                features.get('symptom_count', 0) / 20.0,
                features.get('fever_present', 0),
                features.get('pain_level', 0) / 10.0,
                features.get('duration_days', 0) / 30.0,
            ]
            # Pad to fixed size
            while len(vector) < 10:
                vector.append(0.0)
            feature_vectors.append(vector[:10])
        
        return np.array(feature_vectors)

class GraphSimilarityEngine:
    """High-performance graph-based patient similarity engine"""
    
    def __init__(self):
        self.cache = ProductionCacheManager()
        self.patient_graph = nx.Graph()
        self.symptom_index = {}
        self.demographic_weights = {
            'age': 0.3, 'gender': 0.2, 'location': 0.2, 'comorbidities': 0.3
        }
        self._initialize_sample_graph()
    
    def _initialize_sample_graph(self):
        """Initialize with sample patient data for demonstration"""
        sample_patients = [
            {
                'id': 'p001', 'symptoms': ['fever', 'cough', 'fatigue'],
                'demographics': {'age': 35, 'gender': 'M'}, 'outcome': 'recovered'
            },
            {
                'id': 'p002', 'symptoms': ['headache', 'nausea', 'dizziness'],
                'demographics': {'age': 42, 'gender': 'F'}, 'outcome': 'managed'
            }
        ]
        
        for patient in sample_patients:
            self.add_patient(patient)
    
    def add_patient(self, patient_data: Dict[str, Any]):
        """Add patient to similarity graph"""
        patient_id = patient_data['id']
        
        # Add patient node
        self.patient_graph.add_node(patient_id, **patient_data)
        
        # Connect based on symptom similarity
        for other_id, other_data in self.patient_graph.nodes(data=True):
            if other_id != patient_id:
                similarity = self._calculate_patient_similarity(
                    patient_data, other_data
                )
                if similarity > 0.1:  # Threshold for edge creation
                    self.patient_graph.add_edge(
                        patient_id, other_id, weight=similarity
                    )
    
    async def find_similar_patients(self, query: Dict[str, Any], k: int = 10) -> List[Dict[str, Any]]:
        """Find k most similar patients"""
        cache_key = self.cache.generate_key("similarity", {**query, 'k': k})
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        similarities = []
        
        for patient_id, patient_data in self.patient_graph.nodes(data=True):
            if patient_id.startswith('p'):  # Only compare with existing patients
                similarity = self._calculate_patient_similarity(query, patient_data)
                if similarity > ProductionConfig.SIMILARITY_THRESHOLD:
                    similarities.append((patient_id, similarity, patient_data))
        
        # Sort by similarity and take top k
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = []
        
        for patient_id, similarity, data in similarities[:k]:
            results.append({
                'patient_id': patient_id,
                'similarity_score': round(similarity, 3),
                'matching_symptoms': list(set(query.get('symptoms', [])) & set(data.get('symptoms', []))),
                'demographics': data.get('demographics', {}),
                'outcome': data.get('outcome', 'unknown')
            })
        
        await self.cache.set(cache_key, results)
        return results
    
    def _calculate_patient_similarity(self, patient1: Dict, patient2: Dict) -> float:
        """Calculate comprehensive patient similarity score"""
        # Symptom similarity (Jaccard index)
        symptoms1 = set(patient1.get('symptoms', []))
        symptoms2 = set(patient2.get('symptoms', []))
        
        if not symptoms1 or not symptoms2:
            symptom_similarity = 0.0
        else:
            intersection = len(symptoms1 & symptoms2)
            union = len(symptoms1 | symptoms2)
            symptom_similarity = intersection / union if union > 0 else 0.0
        
        # Demographic similarity
        demo1 = patient1.get('demographics', {})
        demo2 = patient2.get('demographics', {})
        demographic_similarity = self._calculate_demographic_similarity(demo1, demo2)
        
        # Weighted combination
        total_similarity = (
            0.6 * symptom_similarity +
            0.4 * demographic_similarity
        )
        
        return total_similarity
    
    def _calculate_demographic_similarity(self, demo1: Dict, demo2: Dict) -> float:
        """Calculate weighted demographic similarity"""
        if not demo1 or not demo2:
            return 0.5
        
        total_score = 0.0
        total_weight = 0.0
        
        for factor, weight in self.demographic_weights.items():
            if factor in demo1 and factor in demo2:
                if factor == 'age':
                    age_diff = abs(demo1['age'] - demo2['age'])
                    factor_score = max(0, 1 - age_diff / 50.0)
                elif factor == 'gender':
                    factor_score = 1.0 if demo1['gender'] == demo2['gender'] else 0.0
                else:
                    factor_score = 0.5  # Placeholder for other factors
                
                total_score += weight * factor_score
                total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.5

class FederatedLearningEngine:
    """Secure federated learning with differential privacy"""
    
    def __init__(self):
        self.client_models = {}
        self.aggregation_rounds = 0
        self.secure_aggregator = SecureAggregator()
        logger.info("Federated learning engine initialized")
    
    async def client_update(self, client_id: str, model_update: Dict[str, Any]):
        """Process client model update with security checks"""
        # Validate update
        if not self._validate_update(model_update):
            raise ValueError("Invalid model update")
        
        # Store update
        self.client_models[client_id] = {
            'update': model_update,
            'timestamp': datetime.now().isoformat(),
            'client_id': client_id
        }
        
        logger.info(f"Client {client_id} update received")
    
    async def aggregate_updates(self, min_clients: int = 3) -> Optional[Dict[str, Any]]:
        """Secure aggregation of client updates"""
        if len(self.client_models) < min_clients:
            logger.warning(f"Insufficient clients for aggregation: {len(self.client_models)}/{min_clients}")
            return None
        
        # Perform secure aggregation
        aggregated_update = await self.secure_aggregator.aggregate(
            list(self.client_models.values())
        )
        
        self.aggregation_rounds += 1
        self.client_models.clear()
        
        logger.info(f"Aggregation round {self.aggregation_rounds} completed")
        return aggregated_update
    
    def _validate_update(self, update: Dict[str, Any]) -> bool:
        """Validate client update for security and integrity"""
        required_fields = ['model_weights', 'client_info', 'signature']
        return all(field in update for field in required_fields)

class SecureAggregator:
    """Secure aggregation with differential privacy"""
    
    async def aggregate(self, client_updates: List[Dict]) -> Dict[str, Any]:
        """Perform secure aggregation"""
        # Simulate secure aggregation process
        await asyncio.sleep(0.05)  # Placeholder for actual computation
        
        return {
            'aggregated_weights': 'simulated_aggregation',
            'privacy_epsilon': 1.0,
            'round_id': int(time.time()),
            'client_count': len(client_updates)
        }

class MedicalAIBackend:
    """Main orchestrator for medical AI system"""
    
    def __init__(self):
        self.device_manager = SecureDeviceManager()
        self.bert_encoder = OptimizedBERTEncoder()
        self.temporal_engine = TemporalAnalysisEngine()
        self.xgboost_ensemble = XGBoostEnsemble()
        self.similarity_engine = GraphSimilarityEngine()
        self.federated_engine = FederatedLearningEngine()
        self.cache = ProductionCacheManager()
        
        self.metrics = {
            'requests_processed': 0,
            'average_latency': 0.0,
            'cache_hit_rate': 0.0,
            'start_time': datetime.now()
        }
        
        logger.info("Medical AI Backend initialized successfully")
    
    async def comprehensive_analysis(self, request: SymptomRequest) -> AnalysisResponse:
        """Perform comprehensive medical analysis"""
        start_time = time.time()
        analysis_id = f"analysis_{uuid.uuid4().hex[:16]}"
        
        try:
            # Parallel execution of analysis components
            bert_task = self.bert_encoder.encode_batch_async([request.symptoms])
            temporal_task = self.temporal_engine.analyze_temporal_patterns(
                [request.patient_history.get('symptom_timeline', [])]
            )
            similarity_task = self.similarity_engine.find_similar_patients({
                'symptoms': request.symptoms,
                'demographics': request.demographics
            })
            
            # Wait for all tasks
            embedding, temporal_results, similar_patients = await asyncio.gather(
                bert_task, temporal_task, similarity_task,
                return_exceptions=True
            )
            
            # Handle exceptions
            if isinstance(embedding, Exception):
                logger.error(f"BERT encoding failed: {embedding}")
                embedding = np.zeros(768)
            if isinstance(temporal_results, Exception):
                logger.error(f"Temporal analysis failed: {temporal_results}")
                temporal_results = [{"trend": "analysis_error", "urgency": "unknown"}]
            if isinstance(similar_patients, Exception):
                logger.error(f"Similarity search failed: {similar_patients}")
                similar_patients = []
            
            # Generate final results
            results = {
                "symptom_analysis": {
                    "embedding_dimensions": embedding.shape[0],
                    "semantic_clusters": self._extract_semantic_features(embedding)
                },
                "temporal_analysis": temporal_results[0] if temporal_results else {},
                "similar_patients": similar_patients,
                "clinical_interpretation": await self._generate_interpretation(
                    request, embedding, temporal_results[0] if temporal_results else {}
                )
            }
            
            # Calculate performance metrics
            processing_time = time.time() - start_time
            self._update_metrics(processing_time)
            
            return AnalysisResponse(
                success=True,
                analysis_id=analysis_id,
                results=results,
                metadata={
                    "processing_time_ms": round(processing_time * 1000, 2),
                    "model_version": ProductionConfig.MODEL_VERSION,
                    "components_used": ["bert", "temporal", "similarity"]
                },
                warnings=self._generate_warnings(request, results)
            )
            
        except Exception as e:
            logger.error(f"Comprehensive analysis failed: {e}")
            raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")
    
    def _extract_semantic_features(self, embedding: np.ndarray) -> Dict[str, Any]:
        """Extract interpretable features from embeddings"""
        return {
            "magnitude": float(np.linalg.norm(embedding)),
            "key_dimensions": embedding[:5].tolist(),  # First 5 dimensions
            "feature_type": "clinical_symptom_embedding"
        }
    
    async def _generate_interpretation(self, request: SymptomRequest, 
                                     embedding: np.ndarray, 
                                     temporal: Dict[str, Any]) -> Dict[str, Any]:
        """Generate clinical interpretation"""
        return {
            "urgency_level": temporal.get('urgency', 'medium'),
            "recommended_actions": [
                "Consult with healthcare provider",
                "Monitor symptom progression",
                "Consider diagnostic testing based on symptoms"
            ],
            "confidence_metrics": {
                "symptom_coverage": min(1.0, len(request.symptoms) / 20.0),
                "temporal_confidence": temporal.get('confidence', 0.5),
                "overall_confidence": 0.7  # Placeholder
            }
        }
    
    def _generate_warnings(self, request: SymptomRequest, results: Dict[str, Any]) -> List[str]:
        """Generate clinical warnings based on analysis"""
        warnings = []
        
        if len(request.symptoms) > 10:
            warnings.append("Large number of symptoms detected - consider comprehensive evaluation")
        
        urgency = results.get('temporal_analysis', {}).get('urgency', '')
        if urgency == 'high':
            warnings.append("High urgency detected - seek medical attention promptly")
        
        return warnings
    
    def _update_metrics(self, latency: float):
        """Update performance metrics"""
        self.metrics['requests_processed'] += 1
        current_avg = self.metrics['average_latency']
        new_count = self.metrics['requests_processed']
        
        # Exponential moving average
        self.metrics['average_latency'] = (
            current_avg * (new_count - 1) + latency
        ) / new_count
    
    async def get_system_status(self) -> Dict[str, Any]:
        """Comprehensive system status report"""
        device_status = self.device_manager.get_device_status()
        memory_usage = self.device_manager.memory_monitor.get_current_usage()
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "version": ProductionConfig.MODEL_VERSION,
            "system": {
                "uptime_seconds": (datetime.now() - self.metrics['start_time']).total_seconds(),
                "requests_processed": self.metrics['requests_processed'],
                "average_latency_ms": round(self.metrics['average_latency'] * 1000, 2)
            },
            "hardware": device_status,
            "memory": memory_usage,
            "components": {
                "bert_encoder": "active",
                "temporal_engine": "active",
                "similarity_engine": "active",
                "xgboost_ensemble": "active" if self.xgboost_ensemble.models else "placeholder",
                "federated_learning": "active"
            }
        }

# FastAPI Application
app = FastAPI(
    title="Medical Diagnostic AI API - Production",
    description="High-performance medical AI system for clinical decision support",
    version=ProductionConfig.MODEL_VERSION,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, restrict to specific domains
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global system instance
medical_ai: Optional[MedicalAIBackend] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifecycle management"""
    global medical_ai
    
    # Initialize system
    logger.info("Initializing Medical AI System...")
    startup_time = time.time()
    
    medical_ai = MedicalAIBackend()
    
    startup_duration = time.time() - startup_time
    logger.info(f"Medical AI System started in {startup_duration:.2f}s")
    
    # Start background tasks
    background_tasks = set()
    task = asyncio.create_task(_monitor_system_health())
    background_tasks.add(task)
    task.add_done_callback(background_tasks.discard)
    
    yield
    
    # Cleanup
    logger.info("Shutting down Medical AI System...")
    if medical_ai:
        medical_ai.device_manager.memory_monitor = None
    logger.info("Medical AI System shutdown complete")

app.router.lifespan_context = lifespan

async def _monitor_system_health():
    """Background system health monitoring"""
    while True:
        try:
            if medical_ai:
                status = await medical_ai.get_system_status()
                # Log system health periodically
                if status['system']['uptime_seconds'] % 300 < 5:  # Every 5 minutes
                    logger.info(f"System health check: {status}")
            
            await asyncio.sleep(ProductionConfig.HEALTH_CHECK_INTERVAL)
        except Exception as e:
            logger.error(f"Health monitoring error: {e}")
            await asyncio.sleep(ProductionConfig.HEALTH_CHECK_INTERVAL)

def get_medical_ai() -> MedicalAIBackend:
    if medical_ai is None:
        raise HTTPException(
            status_code=503,
            detail="Medical AI system not initialized"
        )
    return medical_ai

# API Endpoints
@app.get("/health", response_model=Dict[str, Any])
async def health_check(ai_system: MedicalAIBackend = Depends(get_medical_ai)):
    """Comprehensive health check endpoint"""
    REQUEST_COUNT.labels(method='GET', endpoint='/health', status='200').inc()
    
    with REQUEST_LATENCY.labels(endpoint='/health').time():
        status = await ai_system.get_system_status()
        
        # Add Prometheus metrics
        CPU_USAGE.set(psutil.cpu_percent())
        MEMORY_USAGE.set(psutil.virtual_memory().used)
        
        if torch.cuda.is_available():
            GPU_MEMORY.set(torch.cuda.memory_allocated(0))
        
        return status

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_symptoms(
    request: SymptomRequest,
    background_tasks: BackgroundTasks,
    ai_system: MedicalAIBackend = Depends(get_medical_ai)
):
    """Comprehensive symptom analysis endpoint"""
    REQUEST_COUNT.labels(method='POST', endpoint='/analyze', status='200').inc()
    
    with REQUEST_LATENCY.labels(endpoint='/analyze').time():
        result = await ai_system.comprehensive_analysis(request)
        
        # Background logging
        background_tasks.add_task(
            logger.info,
            f"Analysis completed: {result.analysis_id}, symptoms: {len(request.symptoms)}"
        )
        
        return result

@app.post("/temporal-analysis")
async def temporal_analysis(
    request: TemporalRequest,
    ai_system: MedicalAIBackend = Depends(get_medical_ai)
):
    """Dedicated temporal analysis endpoint"""
    REQUEST_COUNT.labels(method='POST', endpoint='/temporal-analysis', status='200').inc()
    
    with REQUEST_LATENCY.labels(endpoint='/temporal-analysis').time():
        results = await ai_system.temporal_engine.analyze_temporal_patterns(
            [request.symptom_timeline]
        )
        
        return {
            "success": True,
            "analysis_id": f"temporal_{uuid.uuid4().hex[:12]}",
            "results": results[0] if results else {},
            "patient_id": request.patient_id
        }

@app.post("/similar-patients")
async def find_similar_patients(
    request: SimilarPatientsRequest,
    ai_system: MedicalAIBackend = Depends(get_medical_ai)
):
    """Find clinically similar patients"""
    REQUEST_COUNT.labels(method='POST', endpoint='/similar-patients', status='200').inc()
    
    with REQUEST_LATENCY.labels(endpoint='/similar-patients').time():
        similar_patients = await ai_system.similarity_engine.find_similar_patients(
            {
                "symptoms": request.symptoms,
                "demographics": request.demographics
            },
            k=request.k
        )
        
        return {
            "success": True,
            "similar_patients": similar_patients,
            "query_metadata": {
                "symptoms_count": len(request.symptoms),
                "similarity_threshold": ProductionConfig.SIMILARITY_THRESHOLD
            }
        }

@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    return prometheus_client.generate_latest()

# Exception handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    REQUEST_COUNT.labels(
        method=request.method, 
        endpoint=request.url.path, 
        status=str(exc.status_code)
    ).inc()
    logger.error(f"HTTP error {exc.status_code}: {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"success": False, "error": exc.detail}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    REQUEST_COUNT.labels(
        method=request.method, 
        endpoint=request.url.path, 
        status='500'
    ).inc()
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"success": False, "error": "Internal server error"}
    )

# Additional imports for completeness
from fastapi.responses import JSONResponse
from fastapi import Request

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1,  # GPU systems typically use 1 worker
        log_config=None,
        access_log=True,
        timeout_keep_alive=5,
        limit_max_requests=1000  # Prevent memory leaks
    )